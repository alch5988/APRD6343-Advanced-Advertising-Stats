{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alch5988/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alch5988/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alch5988/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "import itertools\n",
    "import zipfile\n",
    "import json\n",
    "punctuation = string.punctuation\n",
    "stopwordsset = set(stopwords.words(\"english\"))\n",
    "stopwordsset.add(\"'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing urls\n",
    "def removeURL(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return result\n",
    "#Extracting contextual words from a sentence\n",
    "def tokenize(text):\n",
    "    #lower case\n",
    "    text = text.lower()\n",
    "    #split into individual words\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "def stem(tokenizedtext):\n",
    "    rootwords = []\n",
    "    for aword in tokenizedtext:\n",
    "        aword = ps.stem(aword)\n",
    "        rootwords.append(aword)\n",
    "    return rootwords\n",
    "def stopWords(tokenizedtext):\n",
    "    goodwords = []\n",
    "    for aword in tokenizedtext:\n",
    "        if aword not in stopwordsset:\n",
    "            goodwords.append(aword)\n",
    "    return goodwords\n",
    "def lemmatizer(tokenizedtext):\n",
    "    lemmawords = []\n",
    "    for aword in tokenizedtext:\n",
    "        aword = wn.lemmatize(aword)\n",
    "        lemmawords.append(aword)\n",
    "    return lemmawords\n",
    "def removePunctuation(tokenizedtext):\n",
    "    nopunctwords = []\n",
    "    for aword in tokenizedtext:\n",
    "        if aword not in punctuation:\n",
    "            nopunctwords.append(aword)\n",
    "    cleanedwords = []\n",
    "    for aword in nopunctwords:\n",
    "        aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
    "        cleanedwords.append(aword)\n",
    "    return cleanedwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMPDIR = 'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TMPDIR):\n",
    "    os.makedirs(TMPDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetzipfiles = glob.glob('*zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping to tmp directory hbo.zip\n",
      "Unzipping to tmp directory hulu.zip\n",
      "Unzipping to tmp directory netflix.zip\n"
     ]
    }
   ],
   "source": [
    "for tweetzipfile in tweetzipfiles:\n",
    "    with zipfile.ZipFile(tweetzipfile, \"r\") as f:\n",
    "        print(\"Unzipping to tmp directory %s\" % tweetzipfile)\n",
    "        f.extractall(TMPDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n"
     ]
    }
   ],
   "source": [
    "#for each file in directory\n",
    "uniquewords = {}\n",
    "count = 0 \n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(count)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        nourlstext = removeURL(text)\n",
    "        tokenizedtext = tokenize(nourlstext)\n",
    "        nostopwordstext = stopWords(tokenizedtext)\n",
    "        lemmatizedtext = lemmatizer(nostopwordstext)\n",
    "        nopuncttext = removePunctuation(lemmatizedtext)\n",
    "        \n",
    "        for aword in nopuncttext:\n",
    "            if aword in uniquewords:\n",
    "                uniquewords[aword] += 1\n",
    "            if aword not in uniquewords:\n",
    "                uniquewords[aword] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstoinclude = set()\n",
    "wordcount = 0\n",
    "for aword in uniquewords:\n",
    "    if uniquewords[aword] > 50: #mentioned more than 50 times.. we used baseline of 25 in class but need to figure out optimal number for my network specifically\n",
    "        wordcount += 1\n",
    "        wordstoinclude.add(aword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Edge List\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n"
     ]
    }
   ],
   "source": [
    "edgelist = open('semantic.edgelist.for.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "\n",
    "#header for Gephi to specify there are undirected ties between words\n",
    "header = ['Source', 'Target', \"Type\"]\n",
    "csvwriter.writerow(header)\n",
    "\n",
    "print('Writing Edge List')\n",
    "\n",
    "uniquewords = {}\n",
    "count = 0 \n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(count)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        nourlstext = removeURL(text)\n",
    "        tokenizedtext = tokenize(nourlstext)\n",
    "        nostopwordstext = stopWords(tokenizedtext)\n",
    "        lemmatizedtext = lemmatizer(nostopwordstext)\n",
    "        nopuncttext = removePunctuation(lemmatizedtext)\n",
    "        \n",
    "        goodwords = []\n",
    "        for aword in nopuncttext:\n",
    "            if aword in wordstoinclude:\n",
    "                goodwords.append(aword.replace(',',''))\n",
    "        \n",
    "        allcombos = itertools.combinations(goodwords, 2)\n",
    "        for acombo in allcombos:\n",
    "            row = []\n",
    "            for anode in acombo:\n",
    "                row.append(anode)\n",
    "            row.append('Undirected')\n",
    "            csvwriter.writerow(row)\n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shut down temp directory\n",
    "shutil.rmtree(TMPDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
